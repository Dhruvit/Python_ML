{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NN_PyTorch.ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"v1c7omBiKz4v","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":119},"outputId":"f8831ab7-d65d-451a-e358-6ad56b991244","executionInfo":{"status":"ok","timestamp":1569869290250,"user_tz":240,"elapsed":381,"user":{"displayName":"Dhruvit Patel","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBZVnu7Wi8L1YsPV6Lp5jYaOzN5Y3r7osv8XPXHGyo=s64","userId":"13576692562522781356"}}},"source":["import numpy as np\n","import torch\n","\n","#Input Tensor\n","X = torch.Tensor([[1,0,1,0],[1,0,1,1],[0,1,0,1]])\n","\n","#output\n","y = torch.Tensor([[1],[1],[0]])\n","\n","print(X)\n","\n","print(y)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["tensor([[1., 0., 1., 0.],\n","        [1., 0., 1., 1.],\n","        [0., 1., 0., 1.]])\n","tensor([[1.],\n","        [1.],\n","        [0.]])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"i-8Op58hL2SO","colab_type":"text"},"source":["Next, we will define the sigmoid function which will act as the activation function and the derivative of the sigmoid function which will help us in the backpropagation step:"]},{"cell_type":"code","metadata":{"id":"iJ4DSK-RMOkt","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":170},"outputId":"a6d84e8e-3436-4d80-96ce-f1c48080e84e","executionInfo":{"status":"ok","timestamp":1569869215797,"user_tz":240,"elapsed":1799,"user":{"displayName":"Dhruvit Patel","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBZVnu7Wi8L1YsPV6Lp5jYaOzN5Y3r7osv8XPXHGyo=s64","userId":"13576692562522781356"}}},"source":["# Sigmoid function\n","def sigmoid(x):\n","  return 1/(1 + torch.exp(-x))\n","\n","# Derivative of sigmoid function\n","def derivatives_sigmoid(x):\n","  return x * (1-x)\n","\n","#Variable initialization\n","epoch=7000 #Setting training iterations\n","lr=0.1 #Setting learning rate\n","inputlayer_neurons = X.shape[1] #number of features in data set\n","hiddenlayer_neurons = 3 #number of hidden layer neurons\n","output_neurons = 1 #number of neurons in output layer\n","\n","#weight and bias initialization\n","wh=torch.randn(inputlayer_neurons, hiddenlayer_neurons).type(torch.FloatTensor)\n","bh=torch.randn(1, hiddenlayer_neurons).type(torch.FloatTensor)\n","wout=torch.randn(hiddenlayer_neurons, output_neurons)\n","bout=torch.randn(1, output_neurons)\n","\n","for i in range(epoch):\n","    #Forward Propogation\n","    hidden_layer_input1 = torch.mm(X, wh)\n","    hidden_layer_input = hidden_layer_input1 + bh\n","    hidden_layer_activations = sigmoid(hidden_layer_input)\n","\n","    output_layer_input1 = torch.mm(hidden_layer_activations, wout)\n","    output_layer_input = output_layer_input1 + bout\n","    output = sigmoid(output_layer_input1)\n","\n","    #Backpropagation\n","    E = y-output #gradient of error\n","    slope_output_layer = derivatives_sigmoid(output)\n","    slope_hidden_layer = derivatives_sigmoid(hidden_layer_activations)\n","    d_output = E * slope_output_layer\n","    Error_at_hidden_layer = torch.mm(d_output, wout.t())\n","    d_hiddenlayer = Error_at_hidden_layer * slope_hidden_layer\n","    wout += torch.mm(hidden_layer_activations.t(), d_output) *lr\n","    bout += d_output.sum() *lr\n","    wh += torch.mm(X.t(), d_hiddenlayer) *lr\n","    bh += d_output.sum() *lr\n","\n","print('actual :\\n', y, '\\n')\n","print('predicted :\\n', output)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["actual :\n"," tensor([[1.],\n","        [1.],\n","        [0.]]) \n","\n","predicted :\n"," tensor([[0.9855],\n","        [0.9805],\n","        [0.0291]])\n"],"name":"stdout"}]}]}